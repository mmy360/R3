diff --git a/flash-attention b/flash-attention
new file mode 160000
index 0000000..1b8e1e6
--- /dev/null
+++ b/flash-attention
@@ -0,0 +1 @@
+Subproject commit 1b8e1e641c6a179be9a0538b7f40fd595050b735
diff --git a/python/run.py b/python/run.py
new file mode 100644
index 0000000..e602836
--- /dev/null
+++ b/python/run.py
@@ -0,0 +1,46 @@
+from fastapi import FastAPI  
+from pydantic import BaseModel  
+from typing import List, Optional  
+import uvicorn  
+  
+app = FastAPI()  
+  
+# 简单的内存存储  
+storage = {}  
+  
+class BatchSetRequest(BaseModel):  
+    keys: List[str]  
+    data: List[List[float]]  
+  
+class BatchGetRequest(BaseModel):  
+    keys: List[str]  
+  
+class BatchExistsRequest(BaseModel):  
+    keys: List[str]  
+  
+@app.post("/batch_set")  
+async def batch_set(request: BatchSetRequest):  
+    results = []  
+    for key, data in zip(request.keys, request.data):  
+        storage[key] = data  
+        results.append(True)  
+    print(request)
+    return {"results": results}  
+  
+@app.post("/batch_get")  
+async def batch_get(request: BatchGetRequest):  
+    data = [storage.get(key) for key in request.keys]  
+    return {"data": data}  
+  
+@app.post("/batch_exists")  
+async def batch_exists(request: BatchExistsRequest):  
+    consecutive_hits = 0  
+    for key in request.keys:  
+        if key in storage:  
+            consecutive_hits += 1  
+        else:  
+            break  
+    return {"consecutive_hits": consecutive_hits}  
+  
+if __name__ == "__main__":  
+    uvicorn.run(app, host="0.0.0.0", port=8000)
diff --git a/python/sglang/srt/managers/schedule_batch.py b/python/sglang/srt/managers/schedule_batch.py
index ba095da..0172c73 100644
--- a/python/sglang/srt/managers/schedule_batch.py
+++ b/python/sglang/srt/managers/schedule_batch.py
@@ -66,7 +66,7 @@ from sglang.srt.mem_cache.common import (
     evict_from_tree_cache,
 )
 from sglang.srt.mem_cache.mamba_radix_cache import MambaRadixCache
-from sglang.srt.mem_cache.memory_pool import HybridReqToTokenPool, ReqToTokenPool
+from sglang.srt.mem_cache.memory_pool import HybridReqToTokenPool, MoEReqToTokenPool, ReqToTokenPool
 from sglang.srt.mem_cache.radix_cache import RadixKey
 from sglang.srt.mem_cache.swa_radix_cache import SWARadixCache
 from sglang.srt.metrics.collector import SchedulerMetricsCollector, TimeStats
@@ -525,12 +525,17 @@ class Req:
         self.extend_logprob_start_len = 0
         self.last_node: Any = None
         self.last_host_node: Any = None
+        self.moe_last_host_node: Any = None
         self.host_hit_length = 0
+        self.moe_host_hit_length = 0
         # The node to lock until for swa radix tree lock ref
         self.swa_uuid_for_lock: Optional[int] = None
         # The prefix length of the last prefix matching
         self.last_matched_prefix_len: int = 0
-
+        self.moe_last_node: Any = None  # MoE RadixCache 的最后节点  
+        self.moe_prefix_indices = torch.empty((0,), dtype=torch.int64)  # 添加 device
+        self.moe_req_pool_idx: Optional[int] = None  # MoE req_to_token pool 的索引  
+        self.moe_last_matched_prefix_len: int = 0  
         # Whether or not if it is chunked. It increments whenever
         # it is chunked, and decrement whenever chunked request is
         # processed.
@@ -680,7 +685,10 @@ class Req:
         # Whether request reached finished condition
         return self.finished_reason is not None
 
-    def init_next_round_input(self, tree_cache: Optional[BasePrefixCache] = None):
+    def init_next_round_input(
+            self, 
+            tree_cache: Optional[BasePrefixCache] = None,
+            moe_tree_cache: Optional[BasePrefixCache] = None):
         self.fill_ids = self.origin_input_ids + self.output_ids
         input_len = len(self.fill_ids)
         # NOTE: the matched length is at most 1 less than the input length to enable logprob computation
@@ -705,6 +713,16 @@ class Req:
                 ),
             )
             self.last_matched_prefix_len = len(self.prefix_indices)
+        if moe_tree_cache is not None:  
+            (
+                self.moe_prefix_indices,
+                self.moe_last_node,
+                self.moe_last_host_node,
+                self.moe_host_hit_length,
+            ) = moe_tree_cache.match_prefix(  
+                key=RadixKey(token_ids=token_ids, extra_key=self.extra_key)  
+            )  
+            self.moe_last_matched_prefix_len = len(self.moe_prefix_indices)  
         self.extend_input_len = len(self.fill_ids) - len(self.prefix_indices)
 
     # Based on https://github.com/vllm-project/vllm/blob/7a64d24aad69e4d2548aa0bf528d9fe63428ab01/vllm/transformers_utils/detokenizer.py#L194-L313
@@ -936,8 +954,11 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
     # Request, memory pool, and cache
     reqs: List[Req]
     req_to_token_pool: ReqToTokenPool = None
+    moe_req_to_token_pool: ReqToTokenPool = None
     token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
+    moe_token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator = None
     tree_cache: BasePrefixCache = None
+    moe_tree_cache: BasePrefixCache= None
     is_hybrid: bool = False
 
     # Batch configs
@@ -964,6 +985,7 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
     seq_lens_cpu: torch.Tensor = None  # shape: [b], int64
     # The output locations of the KV cache
     out_cache_loc: torch.Tensor = None  # shape: [b], int64
+    moe_routing_cache_loc: torch.Tensor = None
     output_ids: torch.Tensor = None  # shape: [b], int64
 
     # For multimodal inputs
@@ -995,6 +1017,9 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
     prefix_lens: List[int] = None
     extend_lens: List[int] = None
     extend_num_tokens: Optional[int] = None
+    moe_prefix_lens: List[int] = None  # MoE routing cache 的 prefix 长度  
+    moe_extend_lens: List[int] = None  # MoE routing cache 的 extend 长度  
+    moe_extend_num_tokens: Optional[int] = None  # MoE 需要分配的总 token 数    
     decoding_reqs: List[Req] = None
     extend_logprob_start_lens: List[int] = None
     # It comes empty list if logprob is not required.
@@ -1034,8 +1059,11 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
         cls,
         reqs: List[Req],
         req_to_token_pool: ReqToTokenPool,
+        moe_req_to_token_pool: MoEReqToTokenPool,
         token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
+        moe_token_to_kv_pool_allocator: BaseTokenToKVPoolAllocator,
         tree_cache: BasePrefixCache,
+        moe_tree_cache: BasePrefixCache,
         model_config: ModelConfig,
         enable_overlap: bool,
         spec_algorithm: SpeculativeAlgorithm,
@@ -1055,8 +1083,11 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
         return cls(
             reqs=reqs,
             req_to_token_pool=req_to_token_pool,
+            moe_req_to_token_pool = moe_req_to_token_pool,
             token_to_kv_pool_allocator=token_to_kv_pool_allocator,
+            moe_token_to_kv_pool_allocator = moe_token_to_kv_pool_allocator,
             tree_cache=tree_cache,
+            moe_tree_cache=moe_tree_cache, 
             is_hybrid=is_hybrid,
             model_config=model_config,
             enable_overlap=enable_overlap,
@@ -1210,8 +1241,12 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
         self.seq_lens = seq_lens_tensor
         self.seq_lens_cpu = seq_lens_cpu
         self.extend_num_tokens = extend_num_tokens
-
-        # Allocate memory
+        self.moe_prefix_lens = [len(r.moe_prefix_indices) for r in reqs]  
+        self.moe_extend_lens = [  
+            len(r.fill_ids) - self.moe_prefix_lens[i]   
+            for i, r in enumerate(reqs)  
+        ]  
+        self.moe_extend_num_tokens = sum(self.moe_extend_lens) 
         out_cache_loc, req_pool_indices_tensor, req_pool_indices = alloc_for_extend(
             self
         )
@@ -1733,6 +1768,7 @@ class ScheduleBatch(ScheduleBatchDisaggregationDecodeMixin):
             seq_lens=self.seq_lens,
             orig_seq_lens=self.orig_seq_lens,
             out_cache_loc=self.out_cache_loc,
+            moe_routing_cache_loc=self.moe_routing_cache_loc,
             seq_lens_cpu=seq_lens_cpu,
             seq_lens_sum=self.seq_lens_sum,
             return_logprob=self.return_logprob,
@@ -1881,3 +1917,4 @@ class ModelWorkerBatch:
 
     # Whether this batch is prefill-only (no token generation needed)
     is_prefill_only: bool = False
+    moe_routing_cache_loc: Optional[torch.Tensor] = None
diff --git a/python/sglang/srt/managers/schedule_policy.py b/python/sglang/srt/managers/schedule_policy.py
index 288984b..d9c1bda 100644
--- a/python/sglang/srt/managers/schedule_policy.py
+++ b/python/sglang/srt/managers/schedule_policy.py
@@ -83,12 +83,14 @@ class SchedulePolicy:
         self,
         policy: str,
         tree_cache: BasePrefixCache,
+        moe_tree_cache : Optional[BasePrefixCache],        
         enable_hierarchical_cache: bool,
         enable_priority_scheduling: bool,
         schedule_low_priority_values_first: bool,
     ):
         self.policy = self._validate_and_adjust_policy(policy, tree_cache)
         self.tree_cache = tree_cache
+        self.moe_tree_cache = moe_tree_cache
         self.enable_hierarchical_cache = enable_hierarchical_cache
         self.enable_priority_scheduling = enable_priority_scheduling
         self.schedule_low_priority_values_first = schedule_low_priority_values_first
@@ -210,6 +212,11 @@ class SchedulePolicy:
                         RadixKey(token_ids=prefix_ids, extra_key=extra_key),
                         torch.empty(len(prefix_ids), dtype=torch.bool),
                     )
+            r.moe_prefix_indices, r.moe_last_node, r.moe_last_host_node, r.moe_host_hit_length = (
+                self.moe_tree_cache.match_prefix(  
+                    key=RadixKey(token_ids=prefix_ids, extra_key=extra_key)  
+                )  
+            )
         return temporary_deprioritized
 
     @staticmethod
diff --git a/python/sglang/srt/managers/scheduler.py b/python/sglang/srt/managers/scheduler.py
index 78457ab..2294994 100644
--- a/python/sglang/srt/managers/scheduler.py
+++ b/python/sglang/srt/managers/scheduler.py
@@ -144,9 +144,9 @@ from sglang.srt.managers.scheduler_update_weights_mixin import (
 from sglang.srt.managers.session_controller import Session
 from sglang.srt.managers.utils import validate_input_length
 from sglang.srt.mem_cache.chunk_cache import ChunkCache, SWAChunkCache
-from sglang.srt.mem_cache.hiradix_cache import HiRadixCache
+from sglang.srt.mem_cache.hiradix_cache import HiRadixCache, MoEHiRadixCache
 from sglang.srt.mem_cache.mamba_radix_cache import MambaRadixCache
-from sglang.srt.mem_cache.radix_cache import RadixCache
+from sglang.srt.mem_cache.radix_cache import MoERadixCache, RadixCache
 from sglang.srt.mem_cache.swa_radix_cache import SWARadixCache
 from sglang.srt.model_executor.forward_batch_info import PPProxyTensors
 from sglang.srt.parser.reasoning_parser import ReasoningParser
@@ -542,6 +542,7 @@ class Scheduler(
         self.policy = SchedulePolicy(
             self.schedule_policy,
             self.tree_cache,
+            self.moe_tree_cache,
             self.enable_hierarchical_cache,
             self.enable_priority_scheduling,
             self.schedule_low_priority_values_first,
@@ -747,7 +748,7 @@ class Scheduler(
     def init_memory_pool_and_cache(self):
         server_args = self.server_args
 
-        self.req_to_token_pool, self.token_to_kv_pool_allocator = (
+        self.req_to_token_pool, self.token_to_kv_pool_allocator, self.moe_req_to_token_pool, self.moe_token_to_kv_pool_allocator= (
             self.tp_worker.get_memory_pool()
         )
 
@@ -882,6 +883,29 @@ class Scheduler(
 
         embedding_cache_size = int(os.environ.get("SGLANG_VLM_CACHE_SIZE_MB", "100"))
         init_embedding_cache(embedding_cache_size * 1024 * 1024)
+        
+        if True:  
+            self.moe_tree_cache = MoEHiRadixCache(  
+                req_to_token_pool=self.moe_req_to_token_pool,  
+                token_to_kv_pool_allocator=self.moe_token_to_kv_pool_allocator,  
+                tp_cache_group=self.tp_cpu_group,  
+                page_size=self.page_size,  
+                hicache_ratio=server_args.hicache_ratio,  
+                hicache_size=server_args.hicache_size,  
+                hicache_write_policy="write_through",  
+                hicache_io_backend=server_args.hicache_io_backend,  
+                hicache_mem_layout=server_args.hicache_mem_layout,  
+                enable_metrics=self.enable_metrics,  
+                hicache_storage_backend="http",  
+                disable=server_args.disable_radix_cache,  
+            )  
+        else:  
+            self.moe_tree_cache = MoERadixCache(  
+                req_to_token_pool=self.moe_req_to_token_pool,  
+                token_to_kv_pool_allocator=self.moe_token_to_kv_pool_allocator,  
+                page_size=self.page_size,  
+                disable=server_args.disable_radix_cache,  
+            )
 
     def init_disaggregation(self):
         self.transfer_backend = TransferBackend(
@@ -1510,7 +1534,7 @@ class Scheduler(
 
     def _prefetch_kvcache(self, req: Req):
         if self.enable_hicache_storage:
-            req.init_next_round_input(self.tree_cache)
+            req.init_next_round_input(self.tree_cache, self.moe_tree_cache)
             if req.last_node.backuped:
                 # only to initiate the prefetch if the last node is backuped
                 # otherwise, the allocated GPU memory must be locked for integrity
@@ -1970,6 +1994,7 @@ class Scheduler(
 
         if self.enable_hierarchical_cache:
             self.tree_cache.check_hicache_events()
+        self.moe_tree_cache.check_hicache_events()
 
         # Get priority queue
         self.policy.calc_priority(self.waiting_queue)
@@ -2025,8 +2050,8 @@ class Scheduler(
                 if not prefetch_done:
                     # skip staging requests that are ongoing prefetch
                     continue
-
-            req.init_next_round_input(self.tree_cache)
+            req.init_next_round_input(self.tree_cache, 
+                                      self.moe_tree_cache)
             res = adder.add_one_req(
                 req,
                 has_chunked_req=(self.chunked_req is not None),
@@ -2085,8 +2110,11 @@ class Scheduler(
         new_batch = ScheduleBatch.init_new(
             can_run_list,
             self.req_to_token_pool,
+            self.moe_req_to_token_pool,
             self.token_to_kv_pool_allocator,
+            self.moe_token_to_kv_pool_allocator,
             self.tree_cache,
+            self.moe_tree_cache,
             self.model_config,
             self.enable_overlap,
             self.spec_algorithm,
diff --git a/python/sglang/srt/managers/scheduler_output_processor_mixin.py b/python/sglang/srt/managers/scheduler_output_processor_mixin.py
index b238f6c..6969a99 100644
--- a/python/sglang/srt/managers/scheduler_output_processor_mixin.py
+++ b/python/sglang/srt/managers/scheduler_output_processor_mixin.py
@@ -92,6 +92,7 @@ class SchedulerOutputProcessorMixin:
 
                     if req.finished():
                         self.tree_cache.cache_finished_req(req)
+                        self.moe_tree_cache.cache_finished_req(req)  
                         req.time_stats.completion_time = time.perf_counter()
                     elif not batch.decoding_reqs or req not in batch.decoding_reqs:
                         # This updates radix so others can match
@@ -190,6 +191,7 @@ class SchedulerOutputProcessorMixin:
 
                     if req.finished():
                         self.tree_cache.cache_finished_req(req)
+                        self.moe_tree_cache.cache_finished_req(req)  
                     else:
                         self.tree_cache.cache_unfinished_req(req)
                 else:
@@ -318,6 +320,7 @@ class SchedulerOutputProcessorMixin:
                         self.tree_cache.cache_finished_req(req)
                 else:
                     self.tree_cache.cache_finished_req(req)
+                    self.moe_tree_cache.cache_finished_req(req)  
 
                 req.time_stats.completion_time = time.perf_counter()
 
diff --git a/python/sglang/srt/managers/tokenizer_manager.py b/python/sglang/srt/managers/tokenizer_manager.py
index 4b90411..a81b7d0 100644
--- a/python/sglang/srt/managers/tokenizer_manager.py
+++ b/python/sglang/srt/managers/tokenizer_manager.py
@@ -1460,12 +1460,13 @@ class TokenizerManager(TokenizerCommunicatorMixin):
             return
 
         if len(recv_obj.input_token_logprobs_val) > 0:
-            state.input_token_logprobs_val.extend(
-                recv_obj.input_token_logprobs_val[recv_obj_index]
-            )
-            state.input_token_logprobs_idx.extend(
-                recv_obj.input_token_logprobs_idx[recv_obj_index]
-            )
+            if recv_obj.input_token_logprobs_val[recv_obj_index]:
+                state.input_token_logprobs_val.extend(
+                    recv_obj.input_token_logprobs_val[recv_obj_index]
+                )
+                state.input_token_logprobs_idx.extend(
+                    recv_obj.input_token_logprobs_idx[recv_obj_index]
+                )
         state.output_token_logprobs_val.extend(
             recv_obj.output_token_logprobs_val[recv_obj_index]
         )
diff --git a/python/sglang/srt/managers/tp_worker.py b/python/sglang/srt/managers/tp_worker.py
index 0a623d4..b7567ca 100644
--- a/python/sglang/srt/managers/tp_worker.py
+++ b/python/sglang/srt/managers/tp_worker.py
@@ -95,6 +95,8 @@ class BaseTpWorker(ABC):
         return (
             self.model_runner.req_to_token_pool,
             self.model_runner.token_to_kv_pool_allocator,
+            self.model_runner.moe_req_to_token_pool,
+            self.model_runner.moe_token_to_kv_pool_allocator
         )
 
     def update_weights_from_disk(self, recv_req: UpdateWeightFromDiskReqInput):
diff --git a/python/sglang/srt/mem_cache/allocator.py b/python/sglang/srt/mem_cache/allocator.py
index 4fefac9..055c18e 100644
--- a/python/sglang/srt/mem_cache/allocator.py
+++ b/python/sglang/srt/mem_cache/allocator.py
@@ -26,7 +26,7 @@ import torch
 import triton
 import triton.language as tl
 
-from sglang.srt.mem_cache.memory_pool import SWAKVPool
+from sglang.srt.mem_cache.memory_pool import MoETokenToKVPool, SWAKVPool
 from sglang.srt.utils import get_bool_env_var, get_num_new_pages, next_power_of_2
 
 if TYPE_CHECKING:
@@ -578,3 +578,43 @@ class PagedTokenToKVPoolAllocator(BaseTokenToKVPoolAllocator):
 
     def load_cpu_copy(self, kv_cache_cpu, indices):
         return self._kvcache.load_cpu_copy(kv_cache_cpu, indices)
+
+
+class MoERoutingAllocator(BaseTokenToKVPoolAllocator):  
+    """Allocator for MoE routing cache indices."""  
+      
+    def __init__(  
+        self,  
+        size: int,  
+        page_size: int,  
+        dtype: torch.dtype,  
+        device: str,  
+        kvcache: MoETokenToKVPool,  
+        need_sort: bool,  
+    ):  
+        super().__init__(size, page_size, dtype, device, kvcache, need_sort)  
+        self.clear()  
+      
+    def clear(self):  
+        self.free_pages = torch.arange(  
+            1, self.size + 1, dtype=torch.int64, device=self.device  
+        )  
+        self.release_pages = torch.empty((0,), dtype=torch.int64, device=self.device)  
+      
+    def available_size(self):  
+        return len(self.free_pages) + len(self.release_pages)  
+      
+    def alloc(self, need_size: int):  
+        if need_size > len(self.free_pages):  
+            return None  
+        select_index = self.free_pages[:need_size]  
+        self.free_pages = self.free_pages[need_size:]  
+        return select_index  
+      
+    def free(self, free_index: torch.Tensor):  
+        if free_index.numel() == 0:  
+            return  
+        if self.need_sort:  
+            self.release_pages = torch.cat((self.release_pages, free_index))  
+        else:  
+            self.free_pages = torch.cat((self.free_pages, free_index))
\ No newline at end of file
diff --git a/python/sglang/srt/mem_cache/common.py b/python/sglang/srt/mem_cache/common.py
index 979b697..74643a8 100644
--- a/python/sglang/srt/mem_cache/common.py
+++ b/python/sglang/srt/mem_cache/common.py
@@ -11,6 +11,7 @@ from sglang.srt.mem_cache.allocator import SWATokenToKVPoolAllocator
 from sglang.srt.mem_cache.base_prefix_cache import BasePrefixCache
 from sglang.srt.mem_cache.chunk_cache import ChunkCache, SWAChunkCache
 from sglang.srt.mem_cache.memory_pool import HybridReqToTokenPool, ReqToTokenPool
+from sglang.srt.mem_cache.radix_cache import MoERadixCache
 from sglang.srt.server_args import get_global_server_args
 from sglang.srt.utils import support_triton
 
@@ -244,6 +245,18 @@ def evict_from_tree_cache(tree_cache: BasePrefixCache | None, num_tokens: int):
         # Standard allocator
         if allocator.available_size() < num_tokens:
             tree_cache.evict(num_tokens)
+            
+def alloc_moe_routing_slots(  
+    moe_radix_cache: MoERadixCache,  
+    num_tokens: int,  
+) -> torch.Tensor:  
+    allocator = moe_radix_cache.token_to_kv_pool_allocator  
+    # logger.info(f"Allocator available size: {allocator.available_size()}")      
+    if allocator.available_size() < num_tokens:  
+        moe_radix_cache.evict(num_tokens - allocator.available_size())  
+      
+    out_cache_loc = allocator.alloc(num_tokens)        
+    return out_cache_loc
 
 
 def alloc_paged_token_slots_extend(
@@ -374,7 +387,48 @@ def alloc_for_extend(
         prefix_tensors,
         batch.req_to_token_pool,
     )
-
+    if hasattr(batch, 'moe_tree_cache') and batch.moe_tree_cache is not None:  
+        batch.moe_routing_cache_loc = alloc_moe_routing_slots(  
+            batch.moe_tree_cache,  
+            batch.extend_num_tokens  
+        )  
+        moe_prefix_lens_cpu = torch.tensor(batch.moe_prefix_lens, dtype=torch.int64)  
+        moe_extend_lens_cpu = torch.tensor(batch.moe_extend_lens, dtype=torch.int64)  
+        moe_prefix_lens_device = moe_prefix_lens_cpu.to(batch.device, non_blocking=True)  
+        moe_extend_lens_device = moe_extend_lens_cpu.to(batch.device, non_blocking=True)  
+
+        moe_req_pool_indices = alloc_req_slots(batch.moe_req_to_token_pool, bs, batch.reqs)        
+        moe_req_pool_indices_cpu = torch.tensor(moe_req_pool_indices, dtype=torch.int64)  
+        moe_req_pool_indices_device = moe_req_pool_indices_cpu.to(batch.device, non_blocking=True)  
+        for req, idx in zip(batch.reqs, moe_req_pool_indices):  
+            req.moe_req_pool_idx = idx          
+        moe_prefix_tensors = [r.moe_prefix_indices for r in batch.reqs]   
+
+        # logger.info(f"batch.moe_routing_cache_loc: {batch.moe_routing_cache_loc}")
+        # logger.info(f"moe_req_pool_indices_device: {moe_req_pool_indices_device}")
+        # logger.info(f"moe_req_pool_indices_cpu: {moe_req_pool_indices_cpu}")
+        # logger.info(f"moe_prefix_lens_device: {moe_prefix_lens_device}")
+        # logger.info(f"moe_prefix_lens_cpu: {moe_prefix_lens_cpu}")
+        # logger.info(f"batch.seq_lens: {batch.seq_lens}")
+        # logger.info(f"batch.seq_lens_cpu: {batch.seq_lens_cpu}")
+        # logger.info(f"moe_extend_lens_device: {moe_extend_lens_device}")
+        # logger.info(f"moe_extend_lens_cpu: {moe_extend_lens_cpu}")
+        # logger.info(f"moe_prefix_tensors: {moe_prefix_tensors}")
+        # logger.info(f"batch.moe_req_to_token_pool: {batch.moe_req_to_token_pool}")
+
+        write_cache_indices(  
+            batch.moe_routing_cache_loc,  
+            moe_req_pool_indices_device,  
+            moe_req_pool_indices_cpu,  
+            moe_prefix_lens_device,  
+            moe_prefix_lens_cpu,  
+            batch.seq_lens,  
+            batch.seq_lens_cpu,  
+            moe_extend_lens_device,  
+            moe_extend_lens_cpu,  
+            moe_prefix_tensors,  
+            batch.moe_req_to_token_pool,  
+        )  
     return out_cache_loc, req_pool_indices_device, req_pool_indices
 
 
@@ -449,6 +503,30 @@ def alloc_for_decode(batch: ScheduleBatch, token_per_req: int) -> torch.Tensor:
         (batch.req_pool_indices, locs), out_cache_loc.to(torch.int32)
     )
 
+
+    if hasattr(batch, 'moe_tree_cache') and batch.moe_tree_cache is not None:  
+        # 分配 MoE routing cache  
+        moe_routing_cache_loc = alloc_moe_routing_slots(  
+            batch.moe_tree_cache,   
+            bs * token_per_req  
+        )  
+        batch.moe_routing_cache_loc = moe_routing_cache_loc    
+        moe_req_pool_indices = torch.tensor(  
+            [req.moe_req_pool_idx for req in batch.reqs],  
+            dtype=torch.int32,  
+            device=batch.device  
+        )  
+
+        # 写入 req_to_token_pool  
+        if batch.model_config.is_encoder_decoder:  
+            locs = batch.encoder_lens + batch.seq_lens  
+        else:  
+            locs = batch.seq_lens.clone()  
+          
+        batch.moe_tree_cache.req_to_token_pool.write(  
+            (moe_req_pool_indices, locs),  
+            moe_routing_cache_loc.to(torch.int32)  
+        )    
     return out_cache_loc
 
 
diff --git a/python/sglang/srt/mem_cache/hiradix_cache.py b/python/sglang/srt/mem_cache/hiradix_cache.py
index 6ea4e1b..a4c3039 100644
--- a/python/sglang/srt/mem_cache/hiradix_cache.py
+++ b/python/sglang/srt/mem_cache/hiradix_cache.py
@@ -8,16 +8,20 @@ from typing import List, Optional
 import torch
 
 from sglang.srt.managers.cache_controller import HiCacheController, PrefetchOperation
-from sglang.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator
+from sglang.srt.managers.schedule_batch import Req
+from sglang.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator, MoERoutingAllocator
 from sglang.srt.mem_cache.base_prefix_cache import MatchResult
 from sglang.srt.mem_cache.memory_pool import (
     MHATokenToKVPool,
     MLATokenToKVPool,
+    MoEReqToTokenPool,
+    MoETokenToKVPool,
     ReqToTokenPool,
 )
 from sglang.srt.mem_cache.memory_pool_host import (
     MHATokenToKVPoolHost,
     MLATokenToKVPoolHost,
+    MoETopKCachePoolHost,
 )
 from sglang.srt.mem_cache.radix_cache import RadixCache, RadixKey, TreeNode
 from sglang.srt.metrics.collector import StorageMetricsCollector
@@ -71,6 +75,14 @@ class HiRadixCache(RadixCache):
                 page_size,
                 hicache_mem_layout,
             )
+        elif isinstance(self.kv_cache, MoETokenToKVPool):
+            self.token_to_kv_pool_host = MoETopKCachePoolHost(
+                self.kv_cache,
+                hicache_ratio,
+                hicache_size,
+                page_size,
+                hicache_mem_layout
+            )
         else:
             raise ValueError(f"HiRadixCache only supports MHA and MLA yet")
 
@@ -245,12 +257,18 @@ class HiRadixCache(RadixCache):
             if not write_back:
                 # no need to lock nodes if write back
                 self.inc_lock_ref(node)
+            # logger.info(f"L2 write initiated: node_id={node.id}")
+            # logger.info(f"tokens={len(host_indices)}")
+            # logger.info(f"device_indices={node.value.tolist()}")
+            # logger.info(f"host_indices={host_indices.tolist()}" ) 
         else:
             return 0
 
         return len(host_indices)
 
     def write_backup_storage(self, node: TreeNode):
+        # logger.info(f"L3 write triggered: node_id={node.id}, tokens={len(node.host_value)}")  
+
         prefix_keys = (
             node.get_prefix_hash_values(node.parent)
             if self.hicache_storage_pass_prefix_keys
@@ -947,3 +965,104 @@ class HiRadixCache(RadixCache):
         del self.ongoing_prefetch[rid]
         self.cache_controller.append_host_mem_release(host_indices[:completed_tokens])
         self.cache_controller.prefetch_tokens_occupied -= len(token_ids)
+
+
+class MoEHiRadixCache(HiRadixCache): 
+    def __init__(  
+        self,  
+        req_to_token_pool: MoEReqToTokenPool,  
+        token_to_kv_pool_allocator: MoERoutingAllocator,  
+        tp_cache_group: torch.distributed.ProcessGroup,  # 新增  
+        page_size: int,  
+        hicache_ratio: float,  # 新增  
+        hicache_size: int,  # 新增  
+        hicache_write_policy: str,  # 新增  
+        hicache_io_backend: str,  # 新增  
+        hicache_mem_layout: str,  # 新增  
+        enable_metrics: bool,  # 新增  
+        hicache_storage_backend: Optional[str] = None,  # 新增  
+        disable: bool = False,  
+    ):  
+        self.kv_cache = token_to_kv_pool_allocator.get_kvcache()  
+        self.token_to_kv_pool_host = MoETopKCachePoolHost(  
+            self.kv_cache,  
+            hicache_ratio,  
+            hicache_size,  
+            page_size,  
+            hicache_mem_layout,  
+        )  
+          
+        super().__init__(  
+            req_to_token_pool=req_to_token_pool,  
+            token_to_kv_pool_allocator=token_to_kv_pool_allocator,  
+            tp_cache_group=tp_cache_group,  
+            page_size=page_size,  
+            hicache_ratio=hicache_ratio,  
+            hicache_size=hicache_size,  
+            hicache_write_policy=hicache_write_policy,  
+            hicache_io_backend=hicache_io_backend,  
+            hicache_mem_layout=hicache_mem_layout,  
+            enable_metrics=enable_metrics,  
+            hicache_storage_backend=hicache_storage_backend,  
+        )
+    def cache_finished_req(self, req: Req, is_insert: bool = True):  
+        if self.disable or req.moe_req_pool_idx is None:  
+            # 如果有分配的 MoE routing cache,需要释放  
+            if req.moe_req_pool_idx is not None:  
+                # 注意:这里使用 all_token_len 而不是 all_token_len - 1  
+                # 因为 MoE routing cache 为每个 token 都存储了 routing 结果  
+                all_token_len = len(req.origin_input_ids) + len(req.output_ids)  
+                moe_indices = self.req_to_token_pool.req_to_token[  
+                    req.moe_req_pool_idx, :all_token_len  
+                ]  
+                self.token_to_kv_pool_allocator.free(moe_indices)  
+                self.req_to_token_pool.free(req.moe_req_pool_idx)  
+            return  
+    
+        # 2. 计算实际的 token 长度  
+        # 注意:MoE routing cache 不需要 -1,因为每个 token 都有 routing 结果  
+        all_token_len = len(req.origin_input_ids) + len(req.output_ids)  
+        
+        # 3. 获取 MoE routing cache 的索引  
+        moe_indices = self.req_to_token_pool.req_to_token[  
+            req.moe_req_pool_idx, :all_token_len  
+        ]  
+    
+        # 4. 处理页对齐  
+        if self.page_size != 1:  
+            page_aligned_len = all_token_len // self.page_size * self.page_size  
+            page_aligned_moe_indices = moe_indices[:page_aligned_len].to(  
+                dtype=torch.int64, copy=True  
+            )  
+        else:  
+            page_aligned_len = all_token_len  
+            page_aligned_moe_indices = moe_indices.to(dtype=torch.int64, copy=True)  
+    
+        # 5. 获取 token IDs 用作 key  
+        token_ids = (req.origin_input_ids + req.output_ids)[:page_aligned_len]  
+    
+        # 6. 计算已缓存的前缀长度  
+        # 使用 moe_prefix_indices 而不是 prefix_indices  
+        old_prefix_len = len(req.moe_prefix_indices) if req.moe_prefix_indices is not None else 0  
+    
+        # 7. 插入到 RadixTree 或释放内存  
+        if is_insert:  
+            new_prefix_len = self.insert(  
+                RadixKey(token_ids, req.extra_key),  
+                page_aligned_moe_indices,  
+            )  
+            # 释放树中已存在的重复部分  
+            if old_prefix_len < new_prefix_len:  
+                self.token_to_kv_pool_allocator.free(  
+                    moe_indices[old_prefix_len:new_prefix_len]  
+                )  
+        else:  
+            self.token_to_kv_pool_allocator.free(  
+                moe_indices[old_prefix_len:page_aligned_len]  
+            )  
+    
+        if page_aligned_len < all_token_len:  
+            self.token_to_kv_pool_allocator.free(moe_indices[page_aligned_len:])  
+
+        self.req_to_token_pool.free(req.moe_req_pool_idx)  
+        self.dec_lock_ref(req.moe_last_node)
\ No newline at end of file
diff --git a/python/sglang/srt/mem_cache/memory_pool.py b/python/sglang/srt/mem_cache/memory_pool.py
index c468269..989913f 100644
--- a/python/sglang/srt/mem_cache/memory_pool.py
+++ b/python/sglang/srt/mem_cache/memory_pool.py
@@ -1833,3 +1833,138 @@ def copy_all_layer_kv_cache_tiled(
     mask = mask_loc[:, None] & mask_byte[None, :]
     vals = tl.load(src_ptr, mask=mask)
     tl.store(tgt_ptr, vals, mask=mask)
+
+class MoETokenToKVPool(KVCache):  
+    """Cache for MoE top-k expert routing results with memory usage tracking."""  
+      
+    def __init__(  
+        self,  
+        size: int,  
+        page_size: int,  
+        dtype: torch.dtype,  
+        layer_num: int,  
+        device: str,  
+        num_experts_per_tok: int,  
+        enable_memory_saver: bool,  
+        start_layer: Optional[int] = None,  
+        end_layer: Optional[int] = None,  
+    ):  
+        super().__init__(  
+            size,  
+            page_size,  
+            dtype,  
+            layer_num,  
+            device,  
+            enable_memory_saver,  
+            start_layer,  
+            end_layer,  
+        )  
+        self.num_experts_per_tok = num_experts_per_tok  
+          
+        # 添加统计变量  
+        self.num_writes = 0  # 总写入次数  
+        self.total_tokens_written = 0  # 总写入的 token 数  
+          
+        self._create_buffers()  
+        self._finalize_allocation_log(size)  
+      
+    def _create_buffers(self):  
+        with self.memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):  
+            with (  
+                torch.cuda.use_mem_pool(self.custom_mem_pool)  
+                if self.enable_custom_mem_pool  
+                else nullcontext()  
+            ):  
+                self.expert_indices_buffer = [  
+                    torch.zeros(  
+                        (self.size + self.page_size, self.num_experts_per_tok),  
+                        dtype=torch.int32,  
+                        device=self.device,  
+                    )  
+                    for _ in range(self.layer_num)  
+                ]  
+                self.routing_weights_buffer = [  
+                    torch.zeros(  
+                        (self.size + self.page_size, self.num_experts_per_tok),  
+                        dtype=self.dtype,  
+                        device=self.device,  
+                    )  
+                    for _ in range(self.layer_num)  
+                ]  
+      
+    def get_key_buffer(self, layer_id: int):  
+        return self.expert_indices_buffer[layer_id - self.start_layer]  
+      
+    def get_value_buffer(self, layer_id: int):  
+        return self.routing_weights_buffer[layer_id - self.start_layer]  
+      
+    def get_kv_buffer(self, layer_id: int):  
+        return (  
+            self.expert_indices_buffer[layer_id - self.start_layer],  
+            self.routing_weights_buffer[layer_id - self.start_layer],  
+        )  
+      
+    def set_kv_buffer(  
+        self,  
+        layer_id,  
+        loc: torch.Tensor,  
+        cache_expert_indices: torch.Tensor,  
+        cache_routing_weights: torch.Tensor,  
+    ):  
+        layer_id = layer_id  
+        if cache_expert_indices.dtype != torch.int32:  
+            cache_expert_indices = cache_expert_indices.to(torch.int32)  
+        if cache_routing_weights.dtype != self.dtype:  
+            cache_routing_weights = cache_routing_weights.to(self.dtype)  
+        self.expert_indices_buffer[layer_id - self.start_layer][loc] = cache_expert_indices  
+        self.routing_weights_buffer[layer_id - self.start_layer][loc] = cache_routing_weights  
+      
+    def get_kv_size_bytes(self):  
+        indices_size = sum(  
+            get_tensor_size_bytes(buf) for buf in self.expert_indices_buffer  
+        )  
+        weights_size = sum(  
+            get_tensor_size_bytes(buf) for buf in self.routing_weights_buffer  
+        )  
+        return indices_size, weights_size  
+    
+class MoEReqToTokenPool:  
+    """Maps requests to their MoE routing cache locations."""  
+      
+    def __init__(  
+        self,  
+        size: int,  
+        max_context_len: int,  
+        device: str,  
+        enable_memory_saver: bool,  
+    ):  
+        self.size = size  
+        self.max_context_len = max_context_len  
+        self.device = device  
+          
+        memory_saver_adapter = TorchMemorySaverAdapter.create(  
+            enable=enable_memory_saver  
+        )  
+          
+        with memory_saver_adapter.region(GPU_MEMORY_TYPE_KV_CACHE):  
+            self.req_to_token = torch.zeros(  
+                (size, max_context_len), dtype=torch.int32, device=device  
+            )  
+          
+        self.free_slots = list(range(size))  
+      
+    def write(self, indices, values):  
+        self.req_to_token[indices] = values  
+      
+    def alloc(self, need_size: int):  
+        if need_size > len(self.free_slots):  
+            return None  
+        select_index = self.free_slots[:need_size]  
+        self.free_slots = self.free_slots[need_size:]  
+        return select_index  
+      
+    def free(self, free_index):  
+        if isinstance(free_index, int):  
+            self.free_slots.append(free_index)  
+        else:  
+            self.free_slots.extend(free_index)    
\ No newline at end of file
diff --git a/python/sglang/srt/mem_cache/memory_pool_host.py b/python/sglang/srt/mem_cache/memory_pool_host.py
index edfae2c..8139104 100644
--- a/python/sglang/srt/mem_cache/memory_pool_host.py
+++ b/python/sglang/srt/mem_cache/memory_pool_host.py
@@ -7,7 +7,7 @@ from typing import Optional
 import psutil
 import torch
 
-from sglang.srt.mem_cache.memory_pool import KVCache, MHATokenToKVPool, MLATokenToKVPool
+from sglang.srt.mem_cache.memory_pool import KVCache, MHATokenToKVPool, MLATokenToKVPool, MoETokenToKVPool
 from sglang.srt.utils import is_npu, is_xpu
 
 _is_npu = is_npu()
@@ -753,3 +753,286 @@ class MLATokenToKVPoolHost(HostKVCache):
         else:
             raise ValueError(f"Unsupported layout: {self.layout}")
         return ptr_list, element_size_list
+
+
+class MoETopKCachePoolHost(HostKVCache):  
+    device_pool: MoETokenToKVPool  
+      
+    def __init__(  
+        self,  
+        device_pool,  
+        host_to_device_ratio: float,  
+        host_size: int,  
+        page_size: int,  
+        layout: str,  
+        pin_memory: bool = True,  
+        device: str = "cpu",  
+    ):  
+        self.num_experts_per_tok = device_pool.num_experts_per_tok  
+          
+        super().__init__(  
+            device_pool,  
+            host_to_device_ratio,  
+            host_size,  
+            page_size,  
+            layout,  
+            pin_memory,  
+            device,  
+        )  
+          
+        self.expert_indices_data_refs = [  
+            self.expert_indices_buffer[i] for i in range(self.layer_num)  
+        ]  
+        self.routing_weights_data_refs = [  
+            self.routing_weights_buffer[i] for i in range(self.layer_num)  
+        ]  
+          
+        self.expert_indices_data_ptrs = torch.tensor(  
+            [x.data_ptr() for x in self.expert_indices_data_refs],  
+            dtype=torch.uint64,  
+            device=self.device_pool.device,  
+        )  
+        self.routing_weights_data_ptrs = torch.tensor(  
+            [x.data_ptr() for x in self.routing_weights_data_refs],  
+            dtype=torch.uint64,  
+            device=self.device_pool.device,  
+        )  
+  
+    def get_size_per_token(self):  
+        self.layer_num = self.device_pool.layer_num  
+        indices_size = self.num_experts_per_tok * 4  
+        weights_size = self.num_experts_per_tok * self.dtype.itemsize  
+        return (indices_size + weights_size) * self.layer_num  
+  
+    def init_kv_buffer(self):  
+        if self.layout == "layer_first":  
+            expert_indices_dims = (self.layer_num, self.size, self.num_experts_per_tok)  
+            routing_weights_dims = (self.layer_num, self.size, self.num_experts_per_tok)  
+        elif self.layout == "page_first":  
+            expert_indices_dims = (self.size, self.layer_num, self.num_experts_per_tok)  
+            routing_weights_dims = (self.size, self.layer_num, self.num_experts_per_tok)  
+        elif self.layout == "page_first_direct":  
+            expert_indices_dims = (  
+                self.page_num,  
+                self.layer_num,  
+                self.page_size,  
+                self.num_experts_per_tok,  
+            )  
+            routing_weights_dims = (  
+                self.page_num,  
+                self.layer_num,  
+                self.page_size,  
+                self.num_experts_per_tok,  
+            )  
+        else:  
+            raise ValueError(f"Unsupported layout: {self.layout}")  
+          
+        self.token_stride_size_indices = self.num_experts_per_tok * 4  
+        self.token_stride_size_weights = self.num_experts_per_tok * self.dtype.itemsize  
+        self.layout_dim_indices = self.token_stride_size_indices * self.layer_num  
+        self.layout_dim_weights = self.token_stride_size_weights * self.layer_num  
+          
+        self.expert_indices_buffer = torch.empty(  
+            expert_indices_dims,  
+            dtype=torch.int32,  
+            device=self.device,  
+            pin_memory=self.pin_memory,  
+        )  
+          
+        self.routing_weights_buffer = torch.empty(  
+            routing_weights_dims,  
+            dtype=self.dtype,  
+            device=self.device,  
+            pin_memory=self.pin_memory,  
+        )  
+          
+        return (self.expert_indices_buffer, self.routing_weights_buffer)  
+  
+    def backup_from_device_all_layer(  
+        self, device_pool, host_indices, device_indices, io_backend  
+    ):  
+        if io_backend == "kernel":  
+            io_backend = "direct"  
+          
+        if io_backend == "direct":  
+            if self.layout == "layer_first":  
+                for layer_id in range(self.layer_num):  
+                    self.expert_indices_buffer[layer_id][host_indices] = \
+                        device_pool.expert_indices_buffer[layer_id][device_indices].cpu()  
+                    self.routing_weights_buffer[layer_id][host_indices] = \
+                        device_pool.routing_weights_buffer[layer_id][device_indices].cpu()  
+                          
+            elif self.layout in ["page_first", "page_first_direct"]:  
+                for layer_id in range(self.layer_num):  
+                    if self.layout == "page_first":  
+                        self.expert_indices_buffer[host_indices, layer_id, :] = \
+                            device_pool.expert_indices_buffer[layer_id][device_indices].cpu()  
+                        self.routing_weights_buffer[host_indices, layer_id, :] = \
+                            device_pool.routing_weights_buffer[layer_id][device_indices].cpu()  
+                    else:  
+                        for i, (h_idx, d_idx) in enumerate(zip(host_indices.tolist(), device_indices.tolist())):  
+                            page_idx = h_idx // self.page_size  
+                            in_page_idx = h_idx % self.page_size  
+                            self.expert_indices_buffer[page_idx, layer_id, in_page_idx, :] = \
+                                device_pool.expert_indices_buffer[layer_id][d_idx].cpu()  
+                            self.routing_weights_buffer[page_idx, layer_id, in_page_idx, :] = \
+                                device_pool.routing_weights_buffer[layer_id][d_idx].cpu()  
+            else:  
+                raise ValueError(f"Unsupported layout: {self.layout}")  
+        else:  
+            raise ValueError(f"Unsupported IO backend: {io_backend}")  
+  
+    def load_to_device_per_layer(  
+        self, device_pool, host_indices, device_indices, layer_id, io_backend  
+    ):  
+        if io_backend == "kernel":  
+            io_backend = "direct"  
+          
+        if io_backend == "direct":  
+            if self.layout == "layer_first":  
+                device_pool.expert_indices_buffer[layer_id][device_indices] = \
+                    self.expert_indices_buffer[layer_id][host_indices].to(device_pool.device)  
+                device_pool.routing_weights_buffer[layer_id][device_indices] = \
+                    self.routing_weights_buffer[layer_id][host_indices].to(device_pool.device)  
+                      
+            elif self.layout == "page_first":  
+                device_pool.expert_indices_buffer[layer_id][device_indices] = \
+                    self.expert_indices_buffer[host_indices, layer_id, :].to(device_pool.device)  
+                device_pool.routing_weights_buffer[layer_id][device_indices] = \
+                    self.routing_weights_buffer[host_indices, layer_id, :].to(device_pool.device) 
+                      
+            elif self.layout == "page_first_direct":  
+                for i, (h_idx, d_idx) in enumerate(zip(host_indices.tolist(), device_indices.tolist())):  
+                    page_idx = h_idx // self.page_size  
+                    in_page_idx = h_idx % self.page_size  
+                    device_pool.expert_indices_buffer[layer_id][d_idx] = \
+                        self.expert_indices_buffer[page_idx, layer_id, in_page_idx, :].to(device_pool.device)  
+                    device_pool.routing_weights_buffer[layer_id][d_idx] = \
+                        self.routing_weights_buffer[page_idx, layer_id, in_page_idx, :].to(device_pool.device)  
+            else:  
+                raise ValueError(f"Unsupported layout: {self.layout}")  
+        else:  
+            raise ValueError(f"Unsupported IO backend: {io_backend}")  
+  
+    def get_data_page(self, index, flat: bool = True) -> torch.Tensor:  
+        if self.layout == "layer_first":  
+            expert_indices_page = self.expert_indices_buffer[:, index:index+self.page_size, :]  
+            routing_weights_page = self.routing_weights_buffer[:, index:index+self.page_size, :]  
+        elif self.layout == "page_first":  
+            expert_indices_page = self.expert_indices_buffer[index:index+self.page_size, :, :]  
+            routing_weights_page = self.routing_weights_buffer[index:index+self.page_size, :, :]  
+        elif self.layout == "page_first_direct":  
+            real_index = index // self.page_size  
+            expert_indices_page = self.expert_indices_buffer[real_index:real_index+1, :, :, :]  
+            routing_weights_page = self.routing_weights_buffer[real_index:real_index+1, :, :, :]  
+        else:  
+            raise ValueError(f"Unsupported layout: {self.layout}")  
+          
+        if flat:  
+            return torch.cat([  
+                expert_indices_page.flatten().to(self.dtype),  
+                routing_weights_page.flatten()  
+            ])  
+        else:  
+            return (expert_indices_page, routing_weights_page)  
+  
+    def get_dummy_flat_data_page(self) -> torch.Tensor:  
+        expert_indices_dummy = torch.zeros(  
+            (self.layer_num, self.page_size, self.num_experts_per_tok),  
+            dtype=torch.int32,  
+            device=self.device,  
+            pin_memory=self.pin_memory,  
+        )  
+        routing_weights_dummy = torch.zeros(  
+            (self.layer_num, self.page_size, self.num_experts_per_tok),  
+            dtype=self.dtype,  
+            device=self.device,  
+            pin_memory=self.pin_memory,  
+        )  
+        return torch.cat([  
+            expert_indices_dummy.flatten().to(self.dtype),  
+            routing_weights_dummy.flatten()  
+        ])  
+  
+    def set_from_flat_data_page(self, index: int, data_page: torch.Tensor) -> None:  
+        indices_size = self.layer_num * self.page_size * self.num_experts_per_tok  
+        expert_indices_flat = data_page[:indices_size].to(torch.int32)  
+        routing_weights_flat = data_page[indices_size:]  
+          
+        if self.layout == "layer_first":  
+            self.expert_indices_buffer[:, index:index+self.page_size, :] = \
+                expert_indices_flat.reshape(self.layer_num, self.page_size, self.num_experts_per_tok)  
+            self.routing_weights_buffer[:, index:index+self.page_size, :] = \
+                routing_weights_flat.reshape(self.layer_num, self.page_size, self.num_experts_per_tok)  
+                  
+        elif self.layout == "page_first":  
+            self.expert_indices_buffer[index:index+self.page_size, :, :] = \
+                expert_indices_flat.reshape(self.page_size, self.layer_num, self.num_experts_per_tok)  
+            self.routing_weights_buffer[index:index+self.page_size, :, :] = \
+                routing_weights_flat.reshape(self.page_size, self.layer_num, self.num_experts_per_tok)  
+                  
+        elif self.layout == "page_first_direct":  
+            real_index = index // self.page_size  
+            self.expert_indices_buffer[real_index:real_index+1, :, :, :] = \
+                expert_indices_flat.reshape(1, self.layer_num, self.page_size, self.num_experts_per_tok)  
+            self.routing_weights_buffer[real_index:real_index+1, :, :, :] = \
+                routing_weights_flat.reshape(1, self.layer_num, self.page_size, self.num_experts_per_tok)  
+        else:  
+            raise ValueError(f"Unsupported layout: {self.layout}")  
+  
+    def get_page_buffer_meta(self, indices):  
+        assert len(indices) % self.page_size == 0  
+        ptr_list = []  
+        expert_indices_data_ptr = self.expert_indices_buffer.data_ptr()  
+        routing_weights_data_ptr = self.routing_weights_buffer.data_ptr()  
+        indices = indices.tolist()  
+          
+        if self.layout == "layer_first":  
+            for index in range(0, len(indices), self.page_size):  
+                for layer_id in range(self.layer_num):  
+                    indices_ptr = (  
+                        expert_indices_data_ptr  
+                        + indices[index] * self.num_experts_per_tok * 4  
+                        + layer_id * self.size * self.num_experts_per_tok * 4  
+                    )  
+                    ptr_list.append(indices_ptr)  
+                      
+                    weights_ptr = (  
+                        routing_weights_data_ptr  
+                        + indices[index] * self.num_experts_per_tok * self.dtype.itemsize  
+                        + layer_id * self.size * self.num_experts_per_tok * self.dtype.itemsize  
+                    )  
+                    ptr_list.append(weights_ptr)  
+              
+            indices_element_size = self.page_size * self.num_experts_per_tok * 4  
+            weights_element_size = self.page_size * self.num_experts_per_tok * self.dtype.itemsize  
+            element_size_list = []  
+            for _ in range(len(indices) // self.page_size):  
+                for _ in range(self.layer_num):  
+                    element_size_list.append(indices_element_size)  
+                    element_size_list.append(weights_element_size)  
+                      
+        elif self.layout in ["page_first", "page_first_direct"]:  
+            for index in range(0, len(indices), self.page_size):  
+                indices_ptr = (  
+                    expert_indices_data_ptr  
+                    + indices[index] * self.layer_num * self.num_experts_per_tok * 4  
+                )  
+                ptr_list.append(indices_ptr)  
+                  
+                weights_ptr = (  
+                    routing_weights_data_ptr  
+                    + indices[index] * self.layer_num * self.num_experts_per_tok * self.dtype.itemsize  
+                )  
+                ptr_list.append(weights_ptr)  
+              
+            indices_element_size = self.layer_num * self.page_size * self.num_experts_per_tok * 4  
+            weights_element_size = self.layer_num * self.page_size * self.num_experts_per_tok * self.dtype.itemsize  
+            element_size_list = []  
+            for _ in range(len(indices) // self.page_size):  
+                element_size_list.append(indices_element_size)  
+                element_size_list.append(weights_element_size)  
+        else:  
+            raise ValueError(f"Unsupported layout: {self.layout}")  
+          
+        return ptr_list, element_size_list
\ No newline at end of file
diff --git a/python/sglang/srt/mem_cache/radix_cache.py b/python/sglang/srt/mem_cache/radix_cache.py
index af96834..d7b8933 100644
--- a/python/sglang/srt/mem_cache/radix_cache.py
+++ b/python/sglang/srt/mem_cache/radix_cache.py
@@ -1,4 +1,5 @@
 from __future__ import annotations
+import logging
 
 """
 Copyright 2023-2024 SGLang Team
@@ -32,7 +33,7 @@ from sglang.srt.disaggregation.kv_events import (
     BlockRemoved,
     BlockStored,
 )
-from sglang.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator
+from sglang.srt.mem_cache.allocator import BaseTokenToKVPoolAllocator, MoERoutingAllocator
 from sglang.srt.mem_cache.base_prefix_cache import BasePrefixCache, MatchResult
 from sglang.srt.mem_cache.evict_policy import (
     EvictionStrategy,
@@ -42,11 +43,12 @@ from sglang.srt.mem_cache.evict_policy import (
     LRUStrategy,
     MRUStrategy,
 )
-from sglang.srt.mem_cache.memory_pool import ReqToTokenPool
+from sglang.srt.mem_cache.memory_pool import MoEReqToTokenPool, ReqToTokenPool
 
 if TYPE_CHECKING:
     from sglang.srt.managers.schedule_batch import Req
 
+logger = logging.getLogger(__name__)
 
 class RadixKey:
 
@@ -743,6 +745,84 @@ class RadixCache(BasePrefixCache):
         return events
 
 
+class MoERadixCache(RadixCache):  
+    """RadixCache for MoE routing results."""  
+      
+    def __init__(  
+        self,  
+        req_to_token_pool: MoEReqToTokenPool,  
+        token_to_kv_pool_allocator: MoERoutingAllocator,  
+        page_size: int,  
+        disable: bool = False,  
+    ):  
+        super().__init__(  
+            req_to_token_pool=req_to_token_pool,  
+            token_to_kv_pool_allocator=token_to_kv_pool_allocator,  
+            page_size=page_size,  
+            disable=disable,  
+        )  
+
+    def cache_finished_req(self, req: Req, is_insert: bool = True):  
+        if self.disable or req.moe_req_pool_idx is None:  
+            if req.moe_req_pool_idx is not None:  
+                # 注意:这里使用 all_token_len 而不是 all_token_len - 1  
+                # 因为 MoE routing cache 为每个 token 都存储了 routing 结果  
+                all_token_len = len(req.origin_input_ids) + len(req.output_ids)  
+                moe_indices = self.req_to_token_pool.req_to_token[  
+                    req.moe_req_pool_idx, :all_token_len  
+                ]  
+                self.token_to_kv_pool_allocator.free(moe_indices)  
+                self.req_to_token_pool.free(req.moe_req_pool_idx)  
+            return  
+    
+        # 2. 计算实际的 token 长度  
+        # 注意:MoE routing cache 不需要 -1,因为每个 token 都有 routing 结果  
+        all_token_len = len(req.origin_input_ids) + len(req.output_ids)  
+        
+        # 3. 获取 MoE routing cache 的索引  
+        moe_indices = self.req_to_token_pool.req_to_token[  
+            req.moe_req_pool_idx, :all_token_len  
+        ]  
+    
+        # 4. 处理页对齐  
+        if self.page_size != 1:  
+            page_aligned_len = all_token_len // self.page_size * self.page_size  
+            page_aligned_moe_indices = moe_indices[:page_aligned_len].to(  
+                dtype=torch.int64, copy=True  
+            )  
+        else:  
+            page_aligned_len = all_token_len  
+            page_aligned_moe_indices = moe_indices.to(dtype=torch.int64, copy=True)  
+    
+        # 5. 获取 token IDs 用作 key  
+        token_ids = (req.origin_input_ids + req.output_ids)[:page_aligned_len]  
+    
+        # 6. 计算已缓存的前缀长度  
+        # 使用 moe_prefix_indices 而不是 prefix_indices  
+        old_prefix_len = len(req.moe_prefix_indices) if req.moe_prefix_indices is not None else 0  
+    
+        # 7. 插入到 RadixTree 或释放内存  
+        if is_insert:  
+            new_prefix_len = self.insert(  
+                RadixKey(token_ids, req.extra_key),  
+                page_aligned_moe_indices,  
+            )  
+            # 释放树中已存在的重复部分  
+            if old_prefix_len < new_prefix_len:  
+                self.token_to_kv_pool_allocator.free(  
+                    moe_indices[old_prefix_len:new_prefix_len]  
+                )  
+        else:  
+            self.token_to_kv_pool_allocator.free(  
+                moe_indices[old_prefix_len:page_aligned_len]  
+            )  
+    
+        if page_aligned_len < all_token_len:  
+            self.token_to_kv_pool_allocator.free(moe_indices[page_aligned_len:])  
+
+        self.req_to_token_pool.free(req.moe_req_pool_idx)  
+        self.dec_lock_ref(req.moe_last_node)
+
 if __name__ == "__main__":
     tree = RadixCache(None, None, page_size=1, disable=False)
 
diff --git a/python/sglang/srt/mem_cache/storage/backend_factory.py b/python/sglang/srt/mem_cache/storage/backend_factory.py
index 4b195c8..e2c2d27 100644
--- a/python/sglang/srt/mem_cache/storage/backend_factory.py
+++ b/python/sglang/srt/mem_cache/storage/backend_factory.py
@@ -183,6 +183,8 @@ class StorageBackendFactory:
             return backend_class.from_env_config(bytes_per_page, dtype, storage_config)
         elif backend_name == "eic":
             return backend_class(storage_config, mem_pool_host)
+        elif backend_name == "http":  
+            return backend_class(storage_config)        
         else:
             raise ValueError(f"Unknown built-in backend: {backend_name}")
 
@@ -221,3 +223,9 @@ StorageBackendFactory.register_backend(
     "sglang.srt.mem_cache.storage.eic.eic_storage",
     "EICStorage",
 )
+
+StorageBackendFactory.register_backend(  
+    "http",  
+    "sglang.srt.mem_cache.storage.http_backend.storage_http",  
+    "HiCacheHTTP"  
+)
\ No newline at end of file
diff --git a/python/sglang/srt/mem_cache/storage/http_backend/storage_http.py b/python/sglang/srt/mem_cache/storage/http_backend/storage_http.py
new file mode 100644
index 0000000..e689c42
--- /dev/null
+++ b/python/sglang/srt/mem_cache/storage/http_backend/storage_http.py
@@ -0,0 +1,123 @@
+import logging
+
+from typing import TYPE_CHECKING, Any
+from sglang.srt.mem_cache.hicache_storage import HiCacheStorage, HiCacheStorageConfig, HiCacheStorageExtraInfo  
+import requests  
+import torch  
+from typing import List, Optional  
+  
+logger = logging.getLogger(__name__)
+
+class HiCacheHTTP(HiCacheStorage):  
+    def __init__(self, storage_config: HiCacheStorageConfig, server_url: str = "http://localhost:8000", timeout: int = 30):  
+        self.server_url = server_url  
+        self.timeout = timeout  
+        self.tp_rank = storage_config.tp_rank  
+        self.tp_size = storage_config.tp_size  
+        self.is_mla_model = storage_config.is_mla_model  
+      
+    def get(  
+        self,  
+        key: str,  
+        target_location: Optional[Any] = None,  
+        target_sizes: Optional[Any] = None,  
+    ) -> torch.Tensor | None:  
+        try:  
+            response = requests.post(  
+                f"{self.server_url}/get",  
+                json={"key": key},  
+                timeout=self.timeout  
+            )  
+            if response.status_code == 200:  
+                data = response.json()["data"]  
+                if data and target_location is not None:  
+                    target_location.copy_(torch.tensor(data))  
+                    return target_location  
+            return None  
+        except Exception as e:  
+            logger.error(f"HTTP get failed: {e}")  
+            return None  
+      
+    def batch_get(  
+        self,  
+        keys: List[str],  
+        target_locations: Optional[Any] = None,  
+        target_sizes: Optional[Any] = None,  
+    ) -> List[torch.Tensor | None]:  
+        try:  
+            response = requests.post(  
+                f"{self.server_url}/batch_get",  
+                json={"keys": keys},  
+                timeout=self.timeout  
+            )  
+            if response.status_code == 200:  
+                data_list = response.json()["data"]  
+                results = []  
+                for i, data in enumerate(data_list):  
+                    if data and target_locations and i < len(target_locations):  
+                        target_locations[i].copy_(torch.tensor(data))  
+                        results.append(target_locations[i])  
+                    else:  
+                        results.append(None)  
+                return results  
+            return [None] * len(keys)  
+        except Exception as e:  
+            logger.error(f"HTTP batch_get failed: {e}")  
+            return [None] * len(keys)  
+      
+    def set(  
+        self,  
+        key: str,  
+        value: Optional[Any] = None,  
+        target_location: Optional[Any] = None,  
+        target_sizes: Optional[Any] = None,  
+    ) -> bool:  
+        try:  
+            if value is not None:  
+                data = value.cpu().numpy().tolist()  
+            else:  
+                return False  
+              
+            response = requests.post(  
+                f"{self.server_url}/set",  
+                json={"key": key, "data": data},  
+                timeout=self.timeout  
+            )  
+            return response.status_code == 200 and response.json().get("success", False)  
+        except Exception as e:  
+            logger.error(f"HTTP set failed: {e}")  
+            return False  
+      
+    def batch_set(  
+        self,  
+        keys: List[str],  
+        values: Optional[Any] = None,  
+        target_locations: Optional[Any] = None,  
+        target_sizes: Optional[Any] = None,  
+    ) -> bool:  
+        try:  
+            if values is None:  
+                return False  
+              
+            data_list = [v.float().cpu().numpy().tolist() for v in values]  
+            response = requests.post(  
+                f"{self.server_url}/batch_set",  
+                json={"keys": keys, "data": data_list},  
+                timeout=self.timeout  
+            )  
+            return response.status_code == 200 and all(response.json().get("results", []))  
+        except Exception as e:  
+            logger.error(f"HTTP batch_set failed: {e}")  
+            return False  
+      
+    def exists(self, key: str) -> bool:  
+        try:  
+            response = requests.post(  
+                f"{self.server_url}/exists",  
+                json={"key": key},  
+                timeout=self.timeout  
+            )  
+            return response.status_code == 200 and response.json().get("exists", False)  
+        except Exception as e:  
+            logger.error(f"HTTP exists failed: {e}")  
+            return False
\ No newline at end of file
diff --git a/python/sglang/srt/model_executor/forward_batch_info.py b/python/sglang/srt/model_executor/forward_batch_info.py
index 398bb0d..4d25af0 100644
--- a/python/sglang/srt/model_executor/forward_batch_info.py
+++ b/python/sglang/srt/model_executor/forward_batch_info.py
@@ -306,6 +306,8 @@ class ForwardBatch:
     tbo_split_seq_index: Optional[int] = None
     tbo_parent_token_range: Optional[Tuple[int, int]] = None
     tbo_children: Optional[List[ForwardBatch]] = None
+    moe_routing_cache: Optional[KVCache] = None  
+    moe_routing_cache_loc: Optional[torch.Tensor] = None  # 新增!    
 
     @classmethod
     def init_new(
@@ -348,6 +350,8 @@ class ForwardBatch:
             input_embeds=batch.input_embeds,
             token_type_ids=batch.token_type_ids,
             tbo_split_seq_index=batch.tbo_split_seq_index,
+            moe_routing_cache=model_runner.moe_routing_cache,  
+            moe_routing_cache_loc=batch.moe_routing_cache_loc,  # 添加这个  
         )
         device = model_runner.device
 
diff --git a/python/sglang/srt/model_executor/model_runner.py b/python/sglang/srt/model_executor/model_runner.py
index 0c64071..bc87ea8 100644
--- a/python/sglang/srt/model_executor/model_runner.py
+++ b/python/sglang/srt/model_executor/model_runner.py
@@ -23,6 +23,7 @@ import socket
 import threading
 import time
 from collections import defaultdict
+from contextlib import nullcontext
 from dataclasses import dataclass
 from typing import List, Optional, Tuple, Union
 
@@ -83,6 +84,7 @@ from sglang.srt.lora.lora_manager import LoRAManager
 from sglang.srt.lora.lora_registry import LoRARef
 from sglang.srt.mem_cache.allocator import (
     BaseTokenToKVPoolAllocator,
+    MoERoutingAllocator,
     PagedTokenToKVPoolAllocator,
     SWATokenToKVPoolAllocator,
     TokenToKVPoolAllocator,
@@ -96,10 +98,13 @@ from sglang.srt.mem_cache.memory_pool import (
     HybridReqToTokenPool,
     MHATokenToKVPool,
     MLATokenToKVPool,
+    MoEReqToTokenPool,
+    MoETokenToKVPool,
     NSATokenToKVPool,
     ReqToTokenPool,
     SWAKVPool,
 )
+from sglang.srt.mem_cache.radix_cache import MoERadixCache
 from sglang.srt.model_executor.cpu_graph_runner import CPUGraphRunner
 from sglang.srt.model_executor.cuda_graph_runner import CudaGraphRunner
 from sglang.srt.model_executor.forward_batch_info import ForwardBatch, PPProxyTensors
@@ -842,7 +847,7 @@ class ModelRunner:
         with self.memory_saver_adapter.region(
             GPU_MEMORY_TYPE_WEIGHTS,
             enable_cpu_backup=self.server_args.enable_weights_cpu_backup,
-        ):
+        ) if not self.is_draft_worker else nullcontext():
             self.model = get_model(
                 model_config=self.model_config,
                 load_config=self.load_config,
@@ -1891,6 +1896,32 @@ class ModelRunner:
         else:
             assert self.is_draft_worker
 
+        self.moe_routing_cache = MoETokenToKVPool(  
+            size=self.max_total_num_tokens,  
+            page_size=self.page_size,  
+            dtype=self.kv_cache_dtype,  
+            layer_num=self.num_effective_layers,  
+            device=self.device,  
+            num_experts_per_tok=self.model_config.hf_config.num_experts_per_tok,  
+            enable_memory_saver=self.server_args.enable_memory_saver,  
+            start_layer=self.start_layer,  
+            end_layer=self.end_layer,  
+        )
+        self.moe_token_to_kv_pool_allocator = MoERoutingAllocator(  
+            size=self.max_total_num_tokens,  
+            page_size=self.page_size,  
+            dtype=self.kv_cache_dtype,  
+            device=self.device,  
+            kvcache=self.moe_routing_cache,  
+            need_sort=False,  
+        )            
+        self.moe_req_to_token_pool = MoEReqToTokenPool(  
+            size=self.req_to_token_pool.size, 
+            max_context_len=self.model_config.context_len,
+            device=self.device,  
+            enable_memory_saver=self.server_args.enable_memory_saver,  
+        )            
+
         logger.info(
             f"Memory pool end. "
             f"avail mem={get_available_gpu_memory(self.device, self.gpu_id):.2f} GB"
diff --git a/python/sglang/srt/models/deepseek_v2.py b/python/sglang/srt/models/deepseek_v2.py
index 76a9467..4467a2c 100644
--- a/python/sglang/srt/models/deepseek_v2.py
+++ b/python/sglang/srt/models/deepseek_v2.py
@@ -665,6 +665,16 @@ class DeepseekV2MoE(nn.Module):
         use_reduce_scatter: bool = False,
         gemm_output_zero_allocator: BumpAllocator = None,
     ) -> torch.Tensor:
+        router_logits = self.gate(hidden_states, gemm_output_zero_allocator)
+        topk_output = self.topk(hidden_states, router_logits)
+        # logger.info(f"{topk_output.topk_ids}")
+        if forward_batch is not None:          
+            forward_batch.moe_routing_cache.set_kv_buffer(  
+                layer_id=self.layer_id,  
+                loc=forward_batch.moe_routing_cache_loc,  
+                cache_expert_indices=topk_output.topk_ids,  
+                cache_routing_weights=topk_output.topk_weights,  
+            )           
         if not self._enable_a2a_moe:
             DUAL_STREAM_TOKEN_THRESHOLD = 1024
             if (
diff --git a/python/sglang/srt/speculative/eagle_info.py b/python/sglang/srt/speculative/eagle_info.py
index 2eebdb6..56066be 100644
--- a/python/sglang/srt/speculative/eagle_info.py
+++ b/python/sglang/srt/speculative/eagle_info.py
@@ -736,6 +736,10 @@ class EagleDraftInput(SpecInput, EagleDraftInputV2Mixin):
             self.topk_index = self.topk_index[: len(new_indices)]
             self.hidden_states = self.hidden_states[: len(new_indices)]
             self.verified_id = self.verified_id[: len(new_indices)]
+            if self.accept_length is not None:
+                self.accept_length = self.accept_length[: len(new_indices)]
+            if self.accept_length_cpu is not None:
+                self.accept_length_cpu = self.accept_length_cpu[:len(new_indices)]
         else:
             # in some cases(e.g draft_extend), we have not filtered the batch by `unfinished_index`
             self.topk_p = self.topk_p[new_indices]
@@ -770,6 +774,17 @@ class EagleDraftInput(SpecInput, EagleDraftInputV2Mixin):
         self.verified_id = torch.cat([self.verified_id, spec_info.verified_id], axis=0)
         self.topk_p = torch.cat([self.topk_p, spec_info.topk_p])
         self.topk_index = torch.cat([self.topk_index, spec_info.topk_index])
+        if self.accept_length is not None and spec_info.accept_length is not None:
+            self.accept_length = torch.cat([self.accept_length, spec_info.accept_length])
+            self.accept_length_cpu = self.accept_length.tolist()
+        elif self.accept_length is not None:
+            zeros = torch.zeros([spec_info.verified_id.shape[0]],dtype=self.accept_length.dtype,device=self.accept_length.device)
+            self.accept_length = torch.cat([self.accept_length, zeros])
+            self.accept_length_cpu = self.accept_length.tolist()
+        elif spec_info.accept_length is not None:
+            zeros = torch.zeros([self.verified_id.shape[0]],dtype=self.accept_length.dtype,device=self.accept_length.device)
+            self.accept_length = torch.cat([zeros, spec_info.accept_length])
+            self.accept_length_cpu = self.accept_length.tolist()
 
 
 @dataclass
